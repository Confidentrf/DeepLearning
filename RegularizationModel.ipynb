{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOBEGf7KcH/XHYtmYrYGihZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Confidentrf/DeepLearning/blob/main/RegularizationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1eHaQR5123TR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "feats = pd.read_csv('OSI_feats_e3.csv')\n",
        "target = pd.read_csv('OSI_target_e2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.2\n",
        "random_state = 13\n",
        "X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)"
      ],
      "metadata": {
        "id": "ZQ5e29uW28KC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape of X_train: {X_train.shape}')\n",
        "print(f'Shape of y_train: {y_train.shape}')\n",
        "print(f'Shape of X_test: {X_test.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A48UIyek2-IW",
        "outputId": "95848a62-7c53-4ac3-d549-dac9bca013dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (9864, 68)\n",
            "Shape of y_train: (9864, 1)\n",
            "Shape of X_test: (2466, 68)\n",
            "Shape of y_test: (2466, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "help(LogisticRegressionCV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIqJC2EM2_iL",
        "outputId": "f7139033-7592-4e3a-c12d-cdca0cc1e6e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class LogisticRegressionCV in module sklearn.linear_model._logistic:\n",
            "\n",
            "class LogisticRegressionCV(LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
            " |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='deprecated', random_state=None, l1_ratios=None)\n",
            " |  \n",
            " |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
            " |  \n",
            " |  See glossary entry for :term:`cross-validation estimator`.\n",
            " |  \n",
            " |  This class implements logistic regression using liblinear, newton-cg, sag\n",
            " |  or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
            " |  regularization with primal formulation. The liblinear solver supports both\n",
            " |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
            " |  Elastic-Net penalty is only supported by the saga solver.\n",
            " |  \n",
            " |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
            " |  is selected by the cross-validator\n",
            " |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
            " |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
            " |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  Cs : int or list of floats, default=10\n",
            " |      Each of the values in Cs describes the inverse of regularization\n",
            " |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
            " |      in a logarithmic scale between 1e-4 and 1e4.\n",
            " |      Like in support vector machines, smaller values specify stronger\n",
            " |      regularization.\n",
            " |  \n",
            " |  fit_intercept : bool, default=True\n",
            " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
            " |      added to the decision function.\n",
            " |  \n",
            " |  cv : int or cross-validation generator, default=None\n",
            " |      The default cross-validation generator used is Stratified K-Folds.\n",
            " |      If an integer is provided, then it is the number of folds used.\n",
            " |      See the module :mod:`sklearn.model_selection` module for the\n",
            " |      list of possible cross-validation objects.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
            " |  \n",
            " |  dual : bool, default=False\n",
            " |      Dual (constrained) or primal (regularized, see also\n",
            " |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
            " |      is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
            " |      n_samples > n_features.\n",
            " |  \n",
            " |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
            " |      Specify the norm of the penalty:\n",
            " |  \n",
            " |      - `'l2'`: add a L2 penalty term (used by default);\n",
            " |      - `'l1'`: add a L1 penalty term;\n",
            " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
            " |  \n",
            " |      .. warning::\n",
            " |         Some penalties may not work with some solvers. See the parameter\n",
            " |         `solver` below, to know the compatibility between the penalty and\n",
            " |         solver.\n",
            " |  \n",
            " |  scoring : str or callable, default=None\n",
            " |      A string (see model evaluation documentation) or\n",
            " |      a scorer callable object / function with signature\n",
            " |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
            " |      that can be used, look at :mod:`sklearn.metrics`. The\n",
            " |      default scoring option used is 'accuracy'.\n",
            " |  \n",
            " |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
            " |  \n",
            " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
            " |      To choose a solver, you might want to consider the following aspects:\n",
            " |  \n",
            " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
            " |        and 'saga' are faster for large ones;\n",
            " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
            " |        'lbfgs' handle multinomial loss;\n",
            " |      - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n",
            " |        because it does not handle warm-starting.\n",
            " |      - 'liblinear' and 'newton-cholesky' can only handle binary classification\n",
            " |        by default. To apply a one-versus-rest scheme for the multiclass setting\n",
            " |        one can wrapt it with the `OneVsRestClassifier`.\n",
            " |      - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
            " |        especially with one-hot encoded categorical features with rare\n",
            " |        categories. Be aware that the memory usage of this solver has a quadratic\n",
            " |        dependency on `n_features` because it explicitly computes the Hessian\n",
            " |        matrix.\n",
            " |  \n",
            " |      .. warning::\n",
            " |         The choice of the algorithm depends on the penalty chosen and on\n",
            " |         (multinomial) multiclass support:\n",
            " |  \n",
            " |         ================= ============================== ======================\n",
            " |         solver            penalty                        multinomial multiclass\n",
            " |         ================= ============================== ======================\n",
            " |         'lbfgs'           'l2'                           yes\n",
            " |         'liblinear'       'l1', 'l2'                     no\n",
            " |         'newton-cg'       'l2'                           yes\n",
            " |         'newton-cholesky' 'l2',                          no\n",
            " |         'sag'             'l2',                          yes\n",
            " |         'saga'            'elasticnet', 'l1', 'l2'       yes\n",
            " |         ================= ============================== ======================\n",
            " |  \n",
            " |      .. note::\n",
            " |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
            " |         with approximately the same scale. You can preprocess the data with\n",
            " |         a scaler from :mod:`sklearn.preprocessing`.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         Stochastic Average Gradient descent solver.\n",
            " |      .. versionadded:: 0.19\n",
            " |         SAGA solver.\n",
            " |      .. versionadded:: 1.2\n",
            " |         newton-cholesky solver.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      Tolerance for stopping criteria.\n",
            " |  \n",
            " |  max_iter : int, default=100\n",
            " |      Maximum number of iterations of the optimization algorithm.\n",
            " |  \n",
            " |  class_weight : dict or 'balanced', default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If not given, all classes are supposed to have weight one.\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         class_weight == 'balanced'\n",
            " |  \n",
            " |  n_jobs : int, default=None\n",
            " |      Number of CPU cores used during the cross-validation loop.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  verbose : int, default=0\n",
            " |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
            " |      positive number for verbosity.\n",
            " |  \n",
            " |  refit : bool, default=True\n",
            " |      If set to True, the scores are averaged across all folds, and the\n",
            " |      coefs and the C that corresponds to the best score is taken, and a\n",
            " |      final refit is done using these parameters.\n",
            " |      Otherwise the coefs, intercepts and C that correspond to the\n",
            " |      best scores across folds are averaged.\n",
            " |  \n",
            " |  intercept_scaling : float, default=1\n",
            " |      Useful only when the solver 'liblinear' is used\n",
            " |      and self.fit_intercept is set to True. In this case, x becomes\n",
            " |      [x, self.intercept_scaling],\n",
            " |      i.e. a \"synthetic\" feature with constant value equal to\n",
            " |      intercept_scaling is appended to the instance vector.\n",
            " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
            " |  \n",
            " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
            " |      as all other features.\n",
            " |      To lessen the effect of regularization on synthetic feature weight\n",
            " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
            " |  \n",
            " |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
            " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
            " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
            " |      across the entire probability distribution, *even when the data is\n",
            " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
            " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
            " |      and otherwise selects 'multinomial'.\n",
            " |  \n",
            " |      .. versionadded:: 0.18\n",
            " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
            " |      .. versionchanged:: 0.22\n",
            " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
            " |      .. deprecated:: 1.5\n",
            " |         ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n",
            " |         From then on, the recommended 'multinomial' will always be used for\n",
            " |         `n_classes >= 3`.\n",
            " |         Solvers that do not support 'multinomial' will raise an error.\n",
            " |         Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you\n",
            " |         still want to use OvR.\n",
            " |  \n",
            " |  random_state : int, RandomState instance, default=None\n",
            " |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
            " |      Note that this only applies to the solver and not the cross-validation\n",
            " |      generator. See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  l1_ratios : list of float, default=None\n",
            " |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
            " |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
            " |      using ``penalty='l2'``, while 1 is equivalent to using\n",
            " |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
            " |      of L1 and L2.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray of shape (n_classes, )\n",
            " |      A list of class labels known to the classifier.\n",
            " |  \n",
            " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
            " |      Coefficient of the features in the decision function.\n",
            " |  \n",
            " |      `coef_` is of shape (1, n_features) when the given problem\n",
            " |      is binary.\n",
            " |  \n",
            " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
            " |      Intercept (a.k.a. bias) added to the decision function.\n",
            " |  \n",
            " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
            " |      `intercept_` is of shape(1,) when the problem is binary.\n",
            " |  \n",
            " |  Cs_ : ndarray of shape (n_cs)\n",
            " |      Array of C i.e. inverse of regularization parameter values used\n",
            " |      for cross-validation.\n",
            " |  \n",
            " |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
            " |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
            " |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
            " |  \n",
            " |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
            " |      dict with classes as the keys, and the path of coefficients obtained\n",
            " |      during cross-validating across each fold and then across each Cs\n",
            " |      after doing an OvR for the corresponding class as values.\n",
            " |      If the 'multi_class' option is set to 'multinomial', then\n",
            " |      the coefs_paths are the coefficients corresponding to each class.\n",
            " |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
            " |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
            " |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
            " |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
            " |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
            " |  \n",
            " |  scores_ : dict\n",
            " |      dict with classes as the keys, and the values as the\n",
            " |      grid of scores obtained during cross-validating each fold, after doing\n",
            " |      an OvR for the corresponding class. If the 'multi_class' option\n",
            " |      given is 'multinomial' then the same scores are repeated across\n",
            " |      all classes, since this is the multinomial class. Each dict value\n",
            " |      has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
            " |      ``penalty='elasticnet'``.\n",
            " |  \n",
            " |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
            " |      Array of C that maps to the best scores across every class. If refit is\n",
            " |      set to False, then for each class, the best C is the average of the\n",
            " |      C's that correspond to the best scores for each fold.\n",
            " |      `C_` is of shape(n_classes,) when the problem is binary.\n",
            " |  \n",
            " |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
            " |      Array of l1_ratio that maps to the best scores across every class. If\n",
            " |      refit is set to False, then for each class, the best l1_ratio is the\n",
            " |      average of the l1_ratio's that correspond to the best scores for each\n",
            " |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
            " |  \n",
            " |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
            " |      Actual number of iterations for all classes, folds and Cs.\n",
            " |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
            " |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
            " |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  LogisticRegression : Logistic regression without tuning the\n",
            " |      hyperparameter `C`.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
            " |  >>> X, y = load_iris(return_X_y=True)\n",
            " |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
            " |  >>> clf.predict(X[:2, :])\n",
            " |  array([0, 0])\n",
            " |  >>> clf.predict_proba(X[:2, :]).shape\n",
            " |  (2, 3)\n",
            " |  >>> clf.score(X, y)\n",
            " |  0.98...\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LogisticRegressionCV\n",
            " |      LogisticRegression\n",
            " |      sklearn.linear_model._base.LinearClassifierMixin\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.linear_model._base.SparseCoefMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            " |      sklearn.utils._metadata_requests._MetadataRequester\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='deprecated', random_state=None, l1_ratios=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, **params)\n",
            " |      Fit the model according to the given training data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vector, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target vector relative to X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,) default=None\n",
            " |          Array of weights that are assigned to individual samples.\n",
            " |          If not provided, then each sample is given unit weight.\n",
            " |      \n",
            " |      **params : dict\n",
            " |          Parameters to pass to the underlying splitter and scorer.\n",
            " |      \n",
            " |          .. versionadded:: 1.4\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted LogisticRegressionCV estimator.\n",
            " |  \n",
            " |  get_metadata_routing(self)\n",
            " |      Get metadata routing of this object.\n",
            " |      \n",
            " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      .. versionadded:: 1.4\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      routing : MetadataRouter\n",
            " |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
            " |          routing information.\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None, **score_params)\n",
            " |      Score using the `scoring` option on the given test data and labels.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      **score_params : dict\n",
            " |          Parameters to pass to the `score` method of the underlying scorer.\n",
            " |      \n",
            " |          .. versionadded:: 1.4\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Score of self.predict(X) w.r.t. y.\n",
            " |  \n",
            " |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV\n",
            " |      Request metadata passed to the ``fit`` method.\n",
            " |      \n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      The options for each parameter are:\n",
            " |      \n",
            " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            " |      \n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            " |      \n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |      \n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |      \n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |  \n",
            " |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV\n",
            " |      Request metadata passed to the ``score`` method.\n",
            " |      \n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      The options for each parameter are:\n",
            " |      \n",
            " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            " |      \n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            " |      \n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |      \n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |      \n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  param = 'l1_ratio'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from LogisticRegression:\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict logarithm of probability estimates.\n",
            " |      \n",
            " |      The returned estimates for all classes are ordered by the\n",
            " |      label of classes.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Vector to be scored, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the sample for each class in the\n",
            " |          model, where classes are ordered as they are in ``self.classes_``.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Probability estimates.\n",
            " |      \n",
            " |      The returned estimates for all classes are ordered by the\n",
            " |      label of classes.\n",
            " |      \n",
            " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
            " |      the softmax function is used to find the predicted probability of\n",
            " |      each class.\n",
            " |      Else use a one-vs-rest approach, i.e. calculate the probability\n",
            " |      of each class assuming it to be positive using the logistic function\n",
            " |      and normalize these values across all the classes.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Vector to be scored, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the sample for each class in the model,\n",
            " |          where classes are ordered as they are in ``self.classes_``.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Predict confidence scores for samples.\n",
            " |      \n",
            " |      The confidence score for a sample is proportional to the signed\n",
            " |      distance of that sample to the hyperplane.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the confidence scores.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
            " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
            " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
            " |          this class would be predicted.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class labels for samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the predictions.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_pred : ndarray of shape (n_samples,)\n",
            " |          Vector containing the class labels for each sample.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
            " |  \n",
            " |  densify(self)\n",
            " |      Convert coefficient matrix to dense array format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
            " |      default format of ``coef_`` and is required for fitting, so calling\n",
            " |      this method is only required on models that have previously been\n",
            " |      sparsified; otherwise, it is a no-op.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  sparsify(self)\n",
            " |      Convert coefficient matrix to sparse format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
            " |      L1-regularized models can be much more memory- and storage-efficient\n",
            " |      than the usual numpy.ndarray representation.\n",
            " |      \n",
            " |      The ``intercept_`` member is not converted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
            " |      this may actually *increase* memory usage, so use this method with\n",
            " |      care. A rule of thumb is that the number of zero elements, which can\n",
            " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
            " |      to provide significant benefits.\n",
            " |      \n",
            " |      After calling this method, further fitting with the partial_fit\n",
            " |      method (if any) will not work until you call densify.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  __sklearn_clone__(self)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |  \n",
            " |  __init_subclass__(**kwargs) from builtins.type\n",
            " |      Set the ``set_{method}_request`` methods.\n",
            " |      \n",
            " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            " |      looks for the information available in the set default values which are\n",
            " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            " |      from method signatures.\n",
            " |      \n",
            " |      The ``__metadata_request__*`` class attributes are used when a method\n",
            " |      does not explicitly accept a metadata through its arguments or if the\n",
            " |      developer would like to specify a request value for those metadata\n",
            " |      which are different from the default ``None``.\n",
            " |      \n",
            " |      References\n",
            " |      ----------\n",
            " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "Cs = np.logspace(-2, 6, 9)\n",
        "model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', cv=10, solver='liblinear', random_state=42, max_iter=50000)\n",
        "model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, solver= 'saga', random_state=42, max_iter=10000)\n",
        "\n",
        "model_l1.fit(X_train, y_train['Revenue'])\n",
        "model_l2.fit(X_train, y_train['Revenue'])"
      ],
      "metadata": {
        "id": "sxfNK9u33BGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best hyperparameter for l1 regularization model: {model_l1.C_[0]}')\n",
        "print(f'Best hyperparameter for l2 regularization model: {model_l2.C_[0]}')"
      ],
      "metadata": {
        "id": "BEKFE9rM3F3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "y_pred_l2 = model_l2.predict(X_test)"
      ],
      "metadata": {
        "id": "ZMkwuXYG3GWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)\n",
        "accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)\n",
        "print(f'Accuracy of the model with l1 regularization is {accuracy_l1*100:.4f}%')\n",
        "print(f'Accuracy of the model with l2 regularization is {accuracy_l2*100:.4f}%')"
      ],
      "metadata": {
        "id": "uxUhe1Rz3JQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_l1, recall_l1, fscore_l1, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l1, y_true=y_test, average='binary')\n",
        "precision_l2, recall_l2, fscore_l2, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l2, y_true=y_test, average='binary')\n",
        "print(f'l1\\nPrecision: {precision_l1:.4f}\\nRecall: {recall_l1:.4f}\\nfscore: {fscore_l1:.4f}\\n\\n')\n",
        "print(f'l2\\nPrecision: {precision_l2:.4f}\\nRecall: {recall_l2:.4f}\\nfscore: {fscore_l2:.4f}')"
      ],
      "metadata": {
        "id": "OJd51LcC3Mpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l1.coef_[0], X_train.columns.values.tolist()))]\n",
        "for item in coef_list:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "LLdS1LRL3PNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l2.coef_[0], X_train.columns.values.tolist()))]\n",
        "for item in coef_list:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "Y6ZRc40d3P2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MndeW2BR3R03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}